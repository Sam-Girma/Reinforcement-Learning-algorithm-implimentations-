{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Grid World Problem**\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> a classical problem in reinforcement learning where an agent must navigate a grid to reach a goal state while avoiding obstacles. The objective is to find the optimal policy that maximizes the cumulative reward. Now I will implement this problem with the 5 different algorithms shown below.\n"
      ],
      "metadata": {
        "id": "-Nb3Qs9Lxdmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Value Iteration\n"
      ],
      "metadata": {
        "id": "1xggCfQmxkcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dynamic programming algorithm used to compute the optimal value function by iteratively improving the value estimates."
      ],
      "metadata": {
        "id": "iU0zFPr0yvl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use a small 3x4 grid as an example where:\n",
        "\n",
        "- 'S' denotes the start state.\n",
        "- 'G' denotes the goal state.\n",
        "- 'O' denotes obstacles.\n",
        "- ' ' denotes empty cells."
      ],
      "metadata": {
        "id": "xYQANasSzid1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1TdxnK6uOoM",
        "outputId": "a861061d-b089-487c-8089-7ab23b544f33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function:\n",
            "[[-1.9 -1.  -1.   0. ]\n",
            " [-1.   0.  -1.  -1. ]\n",
            " [-1.9 -1.  -1.9 -1.9]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def value_iteration(grid, rewards, gamma=0.9, theta=1e-6):\n",
        "    n, m = grid.shape\n",
        "    V = np.zeros((n, m))\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for i in range(n):\n",
        "            for j in range(m):\n",
        "                if grid[i, j] in ['G', 'O']:\n",
        "                    continue\n",
        "                v = V[i, j]\n",
        "                V[i, j] = max([\n",
        "                    rewards[i, j] + gamma * V[min(n-1, i+1), j],  # Down\n",
        "                    rewards[i, j] + gamma * V[max(0, i-1), j],    # Up\n",
        "                    rewards[i, j] + gamma * V[i, min(m-1, j+1)],  # Right\n",
        "                    rewards[i, j] + gamma * V[i, max(0, j-1)]     # Left\n",
        "                ])\n",
        "                delta = max(delta, abs(v - V[i, j]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def main():\n",
        "    grid = np.array([['S', ' ', ' ', 'G'],\n",
        "                     [' ', 'O', ' ', ' '],\n",
        "                     [' ', ' ', ' ', ' ']])\n",
        "    rewards = np.array([[-1, -1, -1, 10],\n",
        "                        [-1, -10, -1, -1],\n",
        "                        [-1, -1, -1, -1]])\n",
        "    V = value_iteration(grid, rewards)\n",
        "    print(\"Optimal Value Function:\")\n",
        "    print(V)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Policy Iteration"
      ],
      "metadata": {
        "id": "liG80Upez9Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another dynamic programming algorithm that alternates between policy evaluation and policy improvement to find the optimal policy.\n"
      ],
      "metadata": {
        "id": "aJMtNoGd0OVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy, grid, rewards, gamma=0.9, theta=1e-6):\n",
        "    n, m = grid.shape\n",
        "    V = np.zeros((n, m))\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for i in range(n):\n",
        "            for j in range(m):\n",
        "                if grid[i, j] in ['G', 'O']:\n",
        "                    continue\n",
        "                v = V[i, j]\n",
        "                action = policy[i, j]\n",
        "                if action == 0:  # Down\n",
        "                    V[i, j] = rewards[i, j] + gamma * V[min(n-1, i+1), j]\n",
        "                elif action == 1:  # Up\n",
        "                    V[i, j] = rewards[i, j] + gamma * V[max(0, i-1), j]\n",
        "                elif action == 2:  # Right\n",
        "                    V[i, j] = rewards[i, j] + gamma * V[i, min(m-1, j+1)]\n",
        "                elif action == 3:  # Left\n",
        "                    V[i, j] = rewards[i, j] + gamma * V[i, max(0, j-1)]\n",
        "                delta = max(delta, abs(v - V[i, j]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def policy_iteration(grid, rewards, gamma=0.9):\n",
        "    n, m = grid.shape\n",
        "    policy = np.random.choice(4, size=(n, m))\n",
        "    while True:\n",
        "        V = policy_evaluation(policy, grid, rewards)\n",
        "        policy_stable = True\n",
        "        for i in range(n):\n",
        "            for j in range(m):\n",
        "                if grid[i, j] in ['G', 'O']:\n",
        "                    continue\n",
        "                old_action = policy[i, j]\n",
        "                policy[i, j] = np.argmax([\n",
        "                    rewards[i, j] + gamma * V[min(n-1, i+1), j],  # Down\n",
        "                    rewards[i, j] + gamma * V[max(0, i-1), j],    # Up\n",
        "                    rewards[i, j] + gamma * V[i, min(m-1, j+1)],  # Right\n",
        "                    rewards[i, j] + gamma * V[i, max(0, j-1)]     # Left\n",
        "                ])\n",
        "                if old_action != policy[i, j]:\n",
        "                    policy_stable = False\n",
        "        if policy_stable:\n",
        "            break\n",
        "    return policy, V\n",
        "\n",
        "grid = np.array([['S', ' ', ' ', 'G'],\n",
        "                  [' ', 'O', ' ', ' '],\n",
        "                  [' ', ' ', ' ', ' ']])\n",
        "rewards = np.array([[-1, -1, -1, 10],\n",
        "                    [-1, -10, -1, -1],\n",
        "                    [-1, -1, -1, -1]])\n",
        "policy, V = policy_iteration(grid, rewards)\n",
        "print(\"Optimal Policy:\")\n",
        "print(policy)\n",
        "print(\"Optimal Value Function:\")\n",
        "print(V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9qSHOSdz9b7",
        "outputId": "931984d6-b0a5-4050-a19a-0ccf363a660d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "[[0 0 2 1]\n",
            " [2 3 3 1]\n",
            " [1 1 1 1]]\n",
            "Optimal Value Function:\n",
            "[[-1.9 -1.  -1.   0. ]\n",
            " [-1.   0.  -1.  -1. ]\n",
            " [-1.9 -1.  -1.9 -1.9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Q-Learning\n"
      ],
      "metadata": {
        "id": "_0I97hKi0R_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A model-free RL algorithm that learns the action-value function."
      ],
      "metadata": {
        "id": "S2svZRys3H_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(grid, rewards, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=1000):\n",
        "    n, m = grid.shape\n",
        "    Q = np.zeros((n, m, 4))\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        i, j = 0, 0  # Start state\n",
        "        while grid[i, j] != 'G':\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action_idx = np.random.choice(4)\n",
        "            else:\n",
        "                action_idx = np.argmax(Q[i, j])\n",
        "            action = action_list[action_idx]\n",
        "\n",
        "            next_i, next_j = next_state((i, j), action)\n",
        "            reward = rewards[next_i, next_j]\n",
        "            Q[i, j, action_idx] = Q[i, j, action_idx] + alpha * (reward + gamma * np.max(Q[next_i, next_j]) - Q[i, j, action_idx])\n",
        "\n",
        "            i, j = next_i, next_j\n",
        "\n",
        "    return Q\n",
        "\n",
        "\n",
        "# Define the environment\n",
        "grid = np.array([['S', ' ', ' ', 'G'],\n",
        "                 [' ', 'O', ' ', ' '],\n",
        "                 [' ', ' ', ' ', ' ']])\n",
        "\n",
        "# Define rewards\n",
        "rewards = np.array([[-1, -1, -1, 10],\n",
        "                    [-1, -10, -1, -1],\n",
        "                    [-1, -1, -1, -1]])\n",
        "\n",
        "# Define actions\n",
        "actions = {'up': 0, 'down': 1, 'left': 2, 'right': 3}\n",
        "action_list = ['up', 'down', 'left', 'right']\n",
        "\n",
        "# Define state transitions\n",
        "def next_state(state, action):\n",
        "    i, j = state\n",
        "    if action == 'up':\n",
        "        return max(0, i-1), j\n",
        "    elif action == 'down':\n",
        "        return min(grid.shape[0]-1, i+1), j\n",
        "    elif action == 'left':\n",
        "        return i, max(0, j-1)\n",
        "    elif action == 'right':\n",
        "        return i, min(grid.shape[1]-1, j+1)\n",
        "\n",
        "Q = q_learning(grid, rewards)\n",
        "print(\"Q-Table:\")\n",
        "print(Q)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4BZfR0l0V7W",
        "outputId": "08d5845f-990b-4093-b3f8-c7e6449062e5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Table:\n",
            "[[[ 4.4706951   1.98361605  4.2500508   6.2       ]\n",
            "  [ 5.71981895 -6.20951597  4.04018704  8.        ]\n",
            "  [ 7.77337752  5.11080461  5.48634996 10.        ]\n",
            "  [ 0.          0.          0.          0.        ]]\n",
            "\n",
            " [[ 4.1440519  -0.5318209  -0.58519851 -2.99761474]\n",
            "  [-0.19       -0.3138499  -0.02599529  5.20180598]\n",
            "  [ 7.94393946 -0.12032709 -1.64846397  0.1997    ]\n",
            "  [ 3.439       0.          0.          0.        ]]\n",
            "\n",
            " [[-0.41935677 -0.3940399  -0.3940399  -0.54327349]\n",
            "  [-1.         -0.3940399  -0.43038784 -0.38858453]\n",
            "  [ 1.38470587 -0.199      -0.109      -0.1       ]\n",
            "  [-0.1         0.          0.          0.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Epsilon-Greedy Policy"
      ],
      "metadata": {
        "id": "C6D9je1G2GN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A policy that balances exploration and exploitation by choosing a random action with probability epsilon and the best-known action with probability 1-epsilon."
      ],
      "metadata": {
        "id": "2RqERO7T3LF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return np.random.choice(4)\n",
        "    else:\n",
        "        return np.argmax(Q[state])\n",
        "\n",
        "grid = np.array([['S', ' ', ' ', 'G'],\n",
        "                  [' ', 'O', ' ', ' '],\n",
        "                  [' ', ' ', ' ', ' ']])\n",
        "rewards = np.array([[-1, -1, -1, 10],\n",
        "                    [-1, -10, -1, -1],\n",
        "                    [-1, -1, -1, -1]])\n",
        "epsilon = 0.1\n",
        "Q = q_learning(grid, rewards)  # Use Q-learning to get Q-values\n",
        "state = (0, 0)\n",
        "action = epsilon_greedy(Q, state, epsilon)\n",
        "print(\"Chosen Action:\", action)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tqLUW6F2JGy",
        "outputId": "0abceb54-f189-48bb-dce3-0c3b2da44c3e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen Action: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. UCB Algorithm"
      ],
      "metadata": {
        "id": "CW_WN7GS2N6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An algorithm that balances exploration and exploitation using an upper confidence bound."
      ],
      "metadata": {
        "id": "52k-G7wL3NfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning_ucb(grid, rewards, alpha=0.1, gamma=0.9, episodes=1000):\n",
        "    n, m = grid.shape\n",
        "    Q = np.zeros((n, m, 4))\n",
        "    N = np.zeros((n, m, 4))\n",
        "\n",
        "    for t in range(1, episodes+1):\n",
        "        i, j = 0, 0  # Start state\n",
        "        while grid[i, j] != 'G':\n",
        "            action_idx = ucb(Q, N, (i, j), t)\n",
        "            N[i, j, action_idx] += 1\n",
        "            action = action_list[action_idx]\n",
        "\n",
        "            next_i, next_j = next_state((i, j), action)\n",
        "            reward = rewards[next_i, next_j]\n",
        "            Q[i, j, action_idx] = Q[i, j, action_idx] + alpha * (reward + gamma * np.max(Q[next_i, next_j]) - Q[i, j, action_idx])\n",
        "\n",
        "            i, j = next_i, next_j\n",
        "\n",
        "    return Q\n",
        "\n",
        "# Define the environment\n",
        "grid = np.array([['S', ' ', ' ', 'G'],\n",
        "                 [' ', 'O', ' ', ' '],\n",
        "                 [' ', ' ', ' ', ' ']])\n",
        "\n",
        "# Define rewards\n",
        "rewards = np.array([[-1, -1, -1, 10],\n",
        "                    [-1, -10, -1, -1],\n",
        "                    [-1, -1, -1, -1]])\n",
        "\n",
        "# Define actions\n",
        "actions = {'up': 0, 'down': 1, 'left': 2, 'right': 3}\n",
        "action_list = ['up', 'down', 'left', 'right']\n",
        "\n",
        "# Define state transitions\n",
        "def next_state(state, action):\n",
        "    i, j = state\n",
        "    if action == 'up':\n",
        "        return max(0, i-1), j\n",
        "    elif action == 'down':\n",
        "        return min(grid.shape[0]-1, i+1), j\n",
        "    elif action == 'left':\n",
        "        return i, max(0, j-1)\n",
        "    elif action == 'right':\n",
        "        return i, min(grid.shape[1]-1, j+1)\n",
        "\n",
        "# UCB implementation\n",
        "def ucb(Q, N, state, t):\n",
        "    ucb_values = Q[state] + np.sqrt(2 * np.log(t + 1) / (N[state] + 1e-5))\n",
        "    return np.argmax(ucb_values)\n",
        "\n",
        "\n",
        "\n",
        "Q = q_learning_ucb(grid, rewards)\n",
        "print(\"Q-Table:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkhWlfMP2QmV",
        "outputId": "7d5f9eeb-d293-46b1-a4e8-475251c268a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Table:\n",
            "[[[-0.76616999 -0.76565225 -0.76616999  6.2       ]\n",
            "  [-0.29539    -1.909      -0.3277009   8.        ]\n",
            "  [-0.1        -0.1        -0.109      10.        ]\n",
            "  [ 0.          0.          0.          0.        ]]\n",
            "\n",
            " [[-0.62963786 -0.5396941  -0.58519851 -1.9       ]\n",
            "  [-0.20629    -0.19981    -0.109      -0.19171   ]\n",
            "  [ 0.178559   -0.1        -1.009      -0.019     ]\n",
            "  [ 1.9        -0.1        -0.1009     -0.01      ]]\n",
            "\n",
            " [[-0.4107114  -0.3940399  -0.3940399  -0.46468   ]\n",
            "  [-1.909      -0.29701    -0.3204109  -0.28      ]\n",
            "  [-0.19       -0.199      -0.109      -0.19      ]\n",
            "  [-0.1        -0.1        -0.109      -0.1       ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Single-State Multi-Armed Bandit Problem**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> a reinforcement learning problem where an agent chooses from\n",
        "𝐾\n",
        "K actions (or \"arms\"), each providing a stochastic reward, to maximize its cumulative reward over time. The challenge is to balance exploring new actions and exploiting known high-reward actions to develop a strategy that maximizes total reward.\n",
        "\n"
      ],
      "metadata": {
        "id": "1pSRdswg3dnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Value Iteration"
      ],
      "metadata": {
        "id": "TYA6Hk1134i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value Iteration is typically not used for multi-armed bandit problems as it is more suited for Markov Decision Processes (MDPs). It Computes the optimal value function by iteratively updating value estimates based on the expected rewards from each arm."
      ],
      "metadata": {
        "id": "Ojsya35n3fhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def value_iteration_bandit(rewards, gamma=0.9, theta=1e-6):\n",
        "    K = len(rewards)\n",
        "    V = np.zeros(K)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for k in range(K):\n",
        "            v = V[k]\n",
        "            V[k] = rewards[k] + gamma * np.max(V)\n",
        "            delta = max(delta, abs(v - V[k]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "rewards = np.random.normal(0, 1, 10)  # Assume 10 arms with normal rewards\n",
        "V = value_iteration_bandit(rewards)\n",
        "print(\"Optimal Value Function:\")\n",
        "print(V)\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEZuQq-K3vU7",
        "outputId": "f1c8e361-5985-4c31-af0a-aaca936b97fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function:\n",
            "[12.79918108 13.30276888 14.62488109 14.57650006 13.22041244 13.63134782\n",
            " 14.58398371 14.73100793 15.63180703 14.13623874]\n",
            "Optimal Value Function:\n",
            "[[-1.9 -1.  -1.   0. ]\n",
            " [-1.   0.  -1.  -1. ]\n",
            " [-1.9 -1.  -1.9 -1.9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Policy Iteration"
      ],
      "metadata": {
        "id": "BLAJmace37dM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternates between policy evaluation and policy improvement to find the optimal policy for selecting arms."
      ],
      "metadata": {
        "id": "YdimOJOA4AtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def policy_evaluation_bandit(policy, rewards, gamma=0.9, theta=1e-6):\n",
        "    K = len(rewards)\n",
        "    V = np.zeros(K)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for k in range(K):\n",
        "            v = V[k]\n",
        "            V[k] = rewards[k] + gamma * V[policy[k]]\n",
        "            delta = max(delta, abs(v - V[k]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def policy_iteration_bandit(rewards, gamma=0.9):\n",
        "    K = len(rewards)\n",
        "    policy = np.random.choice(K, K)\n",
        "    while True:\n",
        "        V = policy_evaluation_bandit(policy, rewards, gamma)\n",
        "        policy_stable = True\n",
        "        for k in range(K):\n",
        "            old_action = policy[k]\n",
        "            policy[k] = np.argmax(rewards + gamma * V)\n",
        "            if old_action != policy[k]:\n",
        "                policy_stable = False\n",
        "        if policy_stable:\n",
        "            break\n",
        "    return policy, V\n",
        "\n",
        "\n",
        "rewards = np.random.normal(0, 1, 10)  # Assume 10 arms with normal rewards\n",
        "policy, V = policy_iteration_bandit(rewards)\n",
        "print(\"Optimal Policy:\")\n",
        "print(policy)\n",
        "print(\"Optimal Value Function:\")\n",
        "print(V)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2T-S7AV4DU2",
        "outputId": "f5ae8bab-7dba-442e-ee5f-78fc1d098c1f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "[3 3 3 3 3 3 3 3 3 3]\n",
            "Optimal Value Function:\n",
            "[14.29401254 12.80423241 14.01454389 15.34704073 14.01365561 15.11124093\n",
            " 14.32990496 14.75615847 13.80987414 15.24004483]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Q-Learning"
      ],
      "metadata": {
        "id": "YQ1bPrPo4JoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learns the action-value function by updating Q-values based on the rewards received from selecting each arm."
      ],
      "metadata": {
        "id": "ne3--5XS47uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning_bandit(K, rewards, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=1000):\n",
        "    Q = np.zeros(K)\n",
        "    for _ in range(episodes):\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = np.random.choice(K)\n",
        "        else:\n",
        "            action = np.argmax(Q)\n",
        "        reward = rewards[action]\n",
        "        Q[action] = Q[action] + alpha * (reward + gamma * np.max(Q) - Q[action])\n",
        "    return Q\n",
        "\n",
        "def main():\n",
        "    rewards = np.random.normal(0, 1, 10)  # Assume 10 arms with normal rewards\n",
        "    Q = q_learning_bandit(len(rewards), rewards)\n",
        "    print(\"Q-Table:\")\n",
        "    print(Q)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHIxelhL4LTb",
        "outputId": "53c3912f-7306-47c5-8ca9-e6a1ec7c6462"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Table:\n",
            "[ 6.71309595  3.34129343 11.65742196  6.06350381  4.98248368  6.78434214\n",
            "  5.41057989  6.67700878  7.52827967  8.58454842]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Epsilon-Greedy Policy"
      ],
      "metadata": {
        "id": "XiJdqdxp4PR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balances exploration and exploitation by selecting a random arm with probability epsilon and the best-known arm with probability 1-epsilon."
      ],
      "metadata": {
        "id": "1hVoQcFa42nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_bandit(K, rewards, epsilon=0.1, episodes=1000):\n",
        "    Q = np.zeros(K)\n",
        "    N = np.zeros(K)\n",
        "    for _ in range(episodes):\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = np.random.choice(K)\n",
        "        else:\n",
        "            action = np.argmax(Q)\n",
        "        reward = rewards[action]\n",
        "        N[action] += 1\n",
        "        Q[action] = Q[action] + (1 / N[action]) * (reward - Q[action])\n",
        "    return Q\n",
        "\n",
        "def main():\n",
        "    rewards = np.random.normal(0, 1, 10)  # Assume 10 arms with normal rewards\n",
        "    Q = epsilon_greedy_bandit(len(rewards), rewards)\n",
        "    print(\"Q-Table:\")\n",
        "    print(Q)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeB0FsVA4R5I",
        "outputId": "7f1c48ef-523a-4ca5-e78c-bbafb612a2ed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Table:\n",
            "[-0.58892424 -1.44681539  1.01538722 -0.69413363 -1.41262326  0.66675352\n",
            "  0.1621129   0.07888828  0.6576616   0.14150586]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. UCB Algorithm"
      ],
      "metadata": {
        "id": "J8d6n-or4aAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balances exploration and exploitation using an upper confidence bound to select the arm with the highest potential reward."
      ],
      "metadata": {
        "id": "LTFyoLOW4z_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ucb_bandit(K, rewards, episodes=1000):\n",
        "    Q = np.zeros(K)\n",
        "    N = np.zeros(K)\n",
        "    for t in range(1, episodes + 1):\n",
        "        if t <= K:\n",
        "            action = t - 1  # Initialize by playing each arm once\n",
        "        else:\n",
        "            ucb_values = Q + np.sqrt(2 * np.log(t) / (N + 1e-5))\n",
        "            action = np.argmax(ucb_values)\n",
        "        reward = rewards[action]\n",
        "        N[action] += 1\n",
        "        Q[action] = Q[action] + (1 / N[action]) * (reward - Q[action])\n",
        "    return Q\n",
        "\n",
        "def main():\n",
        "    rewards = np.random.normal(0, 1, 10)  # Assume 10 arms with normal rewards\n",
        "    Q = ucb_bandit(len(rewards), rewards)\n",
        "    print(\"Q-Table:\")\n",
        "    print(Q)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaW7aCel4cua",
        "outputId": "6cddd302-6fd4-420a-94ad-14f63fa6bb11"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Table:\n",
            "[-0.23692975  2.22726405  0.60906658  0.85371416  0.67962946  0.0858082\n",
            " -1.60707274 -0.01528898 -0.11568769  0.8332205 ]\n"
          ]
        }
      ]
    }
  ]
}